# [Natural Language Processing Specialization](https://www.coursera.org/specializations/natural-language-processing)
<i>A series of 4 courses offered by deeplearning.ai</i>

### Course 01 - [Natural Language Processing with Classification and Vector Spaces](https://www.coursera.org/learn/classification-vector-spaces-in-nlp)

* Week 1: Logistic Regression

  * Lab 1: [Preprocessing.ipynb](https://github.com/Andrew-Ng-s-number-one-fan/Natural-Language-Processing-Specialization/blob/master/01%20-%20Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/C1_W1_N1_Preprocessing.ipynb)
  * Lab 2: [Building and Visualizing Word Frequencies.ipynb](https://github.com/Andrew-Ng-s-number-one-fan/Natural-Language-Processing-Specialization/blob/master/01%20-%20Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/C1_W1_N2_Building%20and%20Visualizing%20Word%20Frequencies.ipynb)
  * Lab 3: [Visualizing Tweets and the Logistic Regression Model.ipynb](https://github.com/Andrew-Ng-s-number-one-fan/Natural-Language-Processing-Specialization/blob/master/01%20-%20Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/C1_W1_N3_Visualizing%20Tweets%20and%20the%20Logistic%20Regression%20Model.ipynb)
  * Assignment
  
* Week 2: Na√Øve Bayes
  * Lab: [Visualizing Likelihoods and Confidence Ellipses.ipynb]()
  * Assignment
  
* Week 3: Word Embeddings
  * Lab 1: [Linear Algebra in Python with Numpy.ipynb]()
  * Lab 2: [Manipulating Word Embeddings.ipynb]()
  * Lab 3: [Another Explanation about PCA.ipynb]()
  * Assignment
  
* Week 4: Word Translation
  * Lab 1: [Rotation Matrices in R2.ipynb]()
  * Lab 2: [Hash Tables.ipynb]()
  * Assignment

### Course 02 - [Natural Language Processing with Probabilistic Models](https://www.coursera.org/learn/probabilistic-models-in-nlp)

* Week 1: Autocorrect

  * Lab 1: [Building the Vocabulary.ipynb]()
  * Lab 2: [Candidates from Edits.ipynb]()
  * Assignment
  
* Week 2: Part of Speech Tagging

  * Lab 1: [Working with Text Data 1.ipynb]()
  * Lab 2: [Working with Text Data 2.ipynb]()
  * Assignment
  
* Week 3: Autocomplete

  * Lab 1: [Corpus Preprocessing for N-grams.ipynb]()
  * Lab 2: [Building the Language Model.ipynb]()
  * Lab 3: [Language Model Generalization.ipynb]()
  * Assignment
  
* Week 4: Word Embeddings

  * Lab 1: [Data Preparation.ipynb]()
  * Lab 2: [Intro to CBOW Model.ipynb]()
  * Lab 3: [Training the CBOW Model.ipynb]()
  * Lab 4: [Word Embeddings.ipynb]()
  * Lab 5: [Word embeddings Step by Step.ipynb]()
  * Assignment

### Course 03 - [Natural Language Processing with Sequence Models](https://www.coursera.org/learn/sequence-models-in-nlp)

### Course 04 - [Natural Language Processing with Attention Models](https://www.coursera.org/learn/attention-models-in-nlp)
